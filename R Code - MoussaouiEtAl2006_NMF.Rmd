---
title: "MCMC algorithm for NMF by Moussaoui *et al.* (2006)"
author: "Ivo Siekmann"
date: "10/08/2021"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(VGAM)
library(NMF)
```

## Generating test data

Functions for generating the matrices $A$ and $S$ from gamma distributions. 

```{r sampleAS}
generateGammaMatrix <- function(rows, cols, a, b) {
  M <- matrix(nrow = rows, ncol = cols)
  for ( i in 1:nrow(M)) {
    M[i,] <- rgamma(cols, a[i], b[i])
  }
  return (M)
}

generateGammaMatrix(2, 2, rep(1, 2), rep(1e-3,2))
```

A test data set can be generated by sampling matrices $A$ and $S$ and adding noise to the resulting matrix $X$. The function `generateTestData` makes $m$ samples from $n$ sources with $N$ variables. 

```{r testdata}
generateTestData <- function(m, n, N, 
                             alpha=rep(1, N), 
                             beta=rep(1e-3, N), 
                             gamma=rep(1, m),
                             lambda=rep(1e-3, m),
                             sigma=rep(0.1, m)) {
  
  S <- generateGammaMatrix(rows = n, cols = N, 
                           alpha, beta)
  Atrans <- generateGammaMatrix(rows = n, cols = m,
                                gamma, lambda)
  A <- t(Atrans)
  
  X <- A%*%S
  
  Xerr <- matrix(nrow = nrow(X), ncol = ncol(X))
  for(i in 1:nrow(X)) {
    Xerr[i,] <- X[i, ] + rnorm(ncol(X), 
                                  mean=0,
                                  sd=sigma[i])
  }
  
  return(list(X=Xerr, A=A,S=S))
}

testData <- generateTestData(m = 2, n = 2,N = 2)
testData
```

This shows that the results are consistent with the noise level $\sigma$:

```{r}
testData$X - testData$A%*%testData$S
```

## *Case 1:* Assuming all hyperparameters are known

First we develop methods under the assumption that the noise level $\sigma$ and the parameters of the gamma distributions are known. 

### Sampling $S$ 

Calculate error of rows of $S$ from mixing matrix $A$ and standard deviation of errors, $\sigma$, see equation (37), p.4137 in Moussaoui *et al.* (2006)).

```{r sigmaLikeSj}
nmf.sigmaSj <- function(A, sigma=rep(0.1, nrow(A))) { 
  sigmaLike <- numeric(length = ncol(A))
  for (j in 1:ncol(A)) {
    sum <- 0
    for (i in 1:nrow(A)) { 
      # sum <- sum+ sigma[i]**2/A[i,j]**2
      sum <- sum+ A[i,j]**2/sigma[i]**2
    }
    sigmaLike[j] <- 1/sum
    # sigmaLike[j] <- sum
  }
  return(sqrt(sigmaLike))
}

sigmaSj <- nmf.sigmaSj(A = testData$A)
sigmaSj
```

Calculate $X - A S$ when column $j$ of $A$ and row $j$ of $S$ are omitted:

```{r}
nmf.epsilonS <- function(X, A, Sold, Snew, j){
  epsilon <-matrix(nrow = nrow(X), ncol = ncol(X))
  for (t in 1:ncol(X)){
    for (i in 1:nrow(X)){
      first.sum<-0
      second.sum<-0
      if( j > 1 ) {
        for (k in 1:(j-1)){
          first.sum <-first.sum + A[i,k]*Snew[k,t]
        }
      }
      if((j+1) <= ncol(A)) {
        for (k in (j+1):(ncol(A))){
          second.sum<-second.sum + A[i,k]*Sold[k,t]
        }
      }
      epsilon[i,t]<- X[i,t] - first.sum - second.sum
    }
    
  }
  return (epsilon)
}

nmf.epsilonS(X = testData$X, 
            Sold = testData$S, Snew = testData$S,
            A = testData$A, j = 1)
```

This is the same as omitting source $j$:

```{r}
testData$X - testData$A[,-1]%*%t(testData$S[-1,])
# testData$X - testData$A[,-2]%*%t(testData$S[-2,])
```

From $\sigma_{s(j, \cdot)}$ we can calculate $\mu^\text{like}_{s(j,t)}$. A mistake from Moussaoui *et al. (2006) must be fixed. The correct formula must be

\[
\mu = \sigma^{\text{like}}_j \sum_{i=1}^{m} \frac{a_{ij} \epsilon_{it}}{\sigma_i^2}
\]



```{r muS}
nmf.muLikelihoodS <- function(X, A, Sold, Snew, 
                             sigmaLikelihood,
                            sigma,j) {
  muLike <- numeric(ncol(X))
  epsilon<-nmf.epsilonS(X,A,Sold,Snew,j)
  
  for (t in 1:ncol(X)) {
    sum <- 0
    for (i in 1:nrow(X)){
      sum<-sum + A[i,j]*epsilon[i,t]/(sigma[i]^2)
      #sum<-sum + A[i,j]*epsilon[i,t]
       # sum<-sum + epsilon[i,t]/A[i,j]
    }
    #muLike[t] <- (1/(sigmaLikelihood[j]^2))*sum
    muLike[t] <- sum*sigmaLikelihood[j]^2
  }
    return(muLike)
}

muLikeS <- nmf.muLikelihoodS(X = testData$X, 
                            A = testData$A, 
                            Sold = testData$S, 
                            Snew = testData$S,
                            sigmaLikelihood =sigmaSj,
                            sigma = rep(0.1,
                            nrow(testData$S)),
                            j = 1)
muLikeS
```

A vectorised version of this calculation:

```{r}
eps1 <- nmf.epsilonS(X = testData$X, A = testData$A,
                    Sold = testData$S, Snew=testData$S,
                    j = 1)

muLikeS <- t(eps1/0.1^2)%*%testData$A[,1]
for (j in 1:ncol(muLikeS)) {
  muLikeS[,j] <- muLikeS[,j]*(sigmaSj[j]^2)
}
muLikeS
```

From $\sigma_{s(j,\cdot)}$ and $\mu_{s(j,t)}$ the matrix $S$ can be sampled from the folded normal distribution. **Note that sampling for the case that $\alpha \neq 1$ is not implemented, yet!**

```{r}
nmf.sampleS <- function(X, A, Sold, 
                        sigma=rep(0.1, nrow(X) ), 
                        alpha=rep(1, ncol(X)),
                        beta=rep(1e-3, ncol(X))) {
  Snew <- matrix(0, nrow=nrow(Sold), ncol=ncol(Sold))
  sigmaLikelihood <- nmf.sigmaSj(A, sigma)
  for(j in 1:nrow(Sold)) {
    mulike <- nmf.muLikelihoodS(X, A, Sold, Snew,
                               sigmaLikelihood, sigma,j)
    # mulike <- nmf.muLikelihoodS(X, A, Sold, Sold,
    #                            sigmaLikelihood, sigma,j)
    
    muPost <- numeric(length = length(mulike))
    muMax <- numeric(length = length(mulike))
    
    for (t in 1:length(mulike)) {
      muPost[t] <- 
        mulike[t] - beta[j]*sigmaLikelihood[j]^2
      
      Delta <- 
        muPost[t]^2 + 4*sigmaLikelihood[j]^2*(alpha[j] - 1)
        
      if(Delta <=0) {
        muMax[t] <- 0
      }
      else {
        muMax[t] <- 1/2*(muPost[t] + sqrt(Delta))
        if(muMax[t] <= 0) {
          muMax[t]<-0
        }
      }
      
      # Snew[j,t] <- rfoldnorm(1,muMax[t],
      #                        sigmaLikelihood[j])
      # cat("sIJ=",sigmaLikelihood[j], "muIJ=",mulike[t], "\n")
      Snew[j,t] <- rfoldnorm(1,muMax[t],
                             sigmaLikelihood[j])
    }
  }
  return(Snew)
}

# newS<-testData$S
newS<-matrix(rep(100,4), nrow=2)

for(i in 1:10){
newS<-nmf.sampleS(X = testData$X,
            A = testData$A,Sold = newS)
}
newS
```

Because we have used the ``true'' solution as the initial condition the result is very similar to the initial condition.

### Sampling $A$ 

Calculate error of columns of $A$ from source matrix $S$ and standard deviation of errors, $\sigma$, see equation (45), p.4138 in Moussaoui *et al.* (2006)). The formula contains a mistake it should be 

\[
\left[ \sigma^{\text{likel}}_{a_{(i,j)}} \right]^2 = \frac{\left[\sigma^{(r)}_i\right]^2}{\sum_{t=1}^N \left[s^{(r+1)}_{jt} \right]^2}.
\]

(in the article the expressions in the denominator are missing the squares)

As we should according to the Gibbs sampler we use the new matrix $S$ (saved in the variable `newS`) rather than the initial condition `testData$S` used previously.

```{r sigmaLikeAj}
nmf.sigmaAij <- function(S, sigma = rep(0.1, nrow(S))) { 
  sigmaLike <- matrix(nrow = length(sigma),
                      ncol = nrow(S))
  rowSumsS<-rowSums(S^2)
  
  for (i in 1:nrow(sigmaLike)) {
    for (j in 1:ncol(sigmaLike)) { 
      sigmaLike[i,j] <- sigma[i]^2/rowSumsS[j]
    }
  }
  return(sqrt(sigmaLike))
}


sigmaAij <- nmf.sigmaAij(newS #S=testData$S
                         )
sigmaAij
```

Calculate deviation of $X$ and $A\cdot S$, again when omitting source $j$.

```{r epsA}
nmf.epsilonA <- function(X, Aold, Anew, S, j){
  epsilon <-matrix(nrow = nrow(X), ncol = ncol(X))
  for (t in 1:ncol(X)){
    for (i in 1:nrow(X)){
      first.sum<-0
      second.sum<-0
      if( j > 1 ) {
        for (k in 1:(j-1)){
          first.sum <-first.sum + Anew[i,k]*S[k,t]
        }
      }
      if((j+1) <= ncol(Aold)) {
        for (k in (j+1):(ncol(Aold))){
          second.sum<-second.sum + Aold[i,k]*S[k,t]
        }
      }
      epsilon[i,t]<- X[i,t] - first.sum - second.sum
    }
    
  }
  return (epsilon)
}

epsA <-nmf.epsilonA(X = testData$X, 
                    Aold = testData$A, 
                    Anew = testData$A,
                    S = newS, j = 1)
epsA
```

Same calculation in vectorised form.

```{r}
testData$X-testData$A[,-1]%*%t(newS[-1,])
```

Now, $\mu^{\text{like}}_{a(i,j)}$ can be calculated. As above, Moussaoui *et al.* (2006) needs to be corrected. The true formula is:

\[
\mu^{\text{like}}_{a_{(i,j)}}=\left[ \sigma^{\text{likel}}_{a_{(i,j)}} \right]^2 \sum_{t=1}^N \frac{s^{(r+1)}_{jt} \epsilon^{(-j)}_{it}}{\left[\sigma^{(r)}_i\right]^2}.
\]


```{r}
nmf.muLikelihoodA <- function(X, Aold, Anew, S, 
                              sigmaLikelihood, 
                              sigma, j) {
  muLike <- numeric(length = nrow(X))
  epsilon<-nmf.epsilonA(X,Aold,Anew,S,j)
  for (i in 1:nrow(X)){
    sum <- 0
    for(t in 1:ncol(S)) {
      sum<-sum + S[j,t]*epsilon[i,t]/sigma[i]^2 
      # sum<-sum + epsilon[i,t]*S[j,t]^2/sigma[i]^2
       # sum <- sum + S[j,t]*epsilon[i,t]
      # sum <- sum + S[j,t]^2*epsilon[i,t]
       # sum <- sum + S[j,t]/epsilon[i,t]*sigma[i]^2
    }
     # muLike[i] <- (1/sigmaLikelihood[i,j]^2)*sum
  # muLike[i] <- sigmaLikelihood[i,j]*sum # close :-)
  muLike[i] <- sigmaLikelihood[i,j]^2*sum
    # muLike[i] <-sum
  }
  return(muLike)
}

nmf.muLikelihoodA(X = testData$X, 
                  Aold = testData$A, Anew = testData$A,
                  S = testData$S, 
                  sigmaLikelihood = sigmaAij,
                  sigma = rep(0.1,
                            nrow(testData$S)), j = 1)
```


From $\sigma_{a(\cdot, j)}$ and $\mu_{a(i,j)}$ the matrix $S$ can be sampled from the folded normal distribution,
**Note that sampling for the case that $\gamma \neq 1$ is not implemented, yet!**

```{r}
nmf.sampleA <- function(X, Aold, S, 
                            sigma=rep(0.1, nrow(X)),
                            gamma=rep(1, ncol(X)),
                            lambda=rep(1e-3, ncol(X))) {
  Anew <- matrix(0, nrow=nrow(Aold), ncol=ncol(Aold))
  sigmaLikelihood <- nmf.sigmaAij(S, sigma)
  
  for(j in 1:ncol(Aold)) {
    mulike <- nmf.muLikelihoodA(X, Aold, Anew, S, 
                                sigmaLikelihood, 
                                sigma, j)
    
    muPost <- numeric(length = length(mulike))
    muMax <- numeric(length = length(mulike))
    
    for (i in 1:length(mulike)) {
      muPost[i] <- 
        mulike[i] - gamma[j]*sigmaLikelihood[j]^2
      
      Delta <- 
        muPost[i]^2 + 4*sigmaLikelihood[j]^2*(gamma[j] - 1)
        
      if(Delta <0) {
        muMax[i] <- 0
      }
      else {
        muMax[i] <- 1/2*(muPost[i] + sqrt(Delta))
        if(muMax[i] <=0) {
          muMax[i]<-0
        }
      }
      
      Anew[i,j] <-
        rfoldnorm(1,muMax[i],sigmaLikelihood[i,j])
    }
  }
  return(Anew)
}

newA <- matrix(rep(100,4), nrow = 2)

for( i in 1:100){
newA <- nmf.sampleA(X = testData$X,Aold = newA,
               S = newS)
}
newA
```


Finally, we verify that the product of the new samples for $S$ and $A$ remains close to $X$:
```{r}
testData$X- newA%*%newS
```

### Gibbs sampler for $A$ and $S$

We can now implement a function that samples $A$ and $S$ under the assumption that all hyperparameters are known.

```{r}
nmf.nextGibbsAS <- function(X, A, S, 
                            sigma=rep(0.1, nrow(X)),
                            alpha=rep(1, ncol(X)),
                            beta=rep(1e-3, ncol(X)),
                            gamma=rep(1, ncol(X)),
                            lambda=rep(1e-3, ncol(X))) {
  nextS <- nmf.sampleS(X,A,S,sigma,alpha, beta)
  
  nextA <- nmf.sampleA(X, A, nextS, sigma, gamma, lambda)

  return ( list(S=nextS, A=nextA))
}

nmf.nextGibbsAS(testData$X, testData$A, testData$S)
```

The function `nmfGibbsAS` runs the Gibbs sampler, assuming all hyperparameters are known for `N` iterations.

```{r}
nmfGibbsAS <- function(X, A, S, 
                     sigma=rep(0.1, nrow(X)),
                            alpha=rep(1, ncol(X)),
                            beta=rep(1e-3, ncol(X)),
                            gamma=rep(1, ncol(X)),
                            lambda=rep(1e-3, ncol(X)),
                     N=10) {
  currSample <- list(S=S, A=A)
  out <-list(currSample)
  
  for(i in 2:(N+1)) {
    currSample <- nmf.nextGibbsAS(
      X, currSample$A, currSample$S,
      sigma,
      alpha, beta, gamma, lambda)
    out[[i]] <- currSample
  }
  return(out)
}

nmfGibbsAS(testData$X, testData$A, testData$S, N=10)
```

We now start with an initial condition far from the ``true'' values `testData$S` and ``testData$A` and run the Gibbs sampler for more iterations.

```{r}
testAS<-nmfGibbsAS(testData$X, matrix(rep(1100,4),nrow = 2), matrix(rep(1000,4),nrow = 2), N=1e6)

lastAS<-testAS[length(testAS)][[1]]

lastAS
```

The result is *not* similar to `testData$S` and `testData$A` (remember that the solution of NMF are usually not unique!). But the resulting product $A\cdot S$ is close to $X$ as it should: 

```{r}
testData$X - testData$A%*%testData$S
```


```{r}
testData$X - lastAS$A%*%lastAS$S
```

In order to be able to easily obtain convergence plots, generate histograms etc. we implement a function for converting the list of samples to a data frame:

```{r}
nmfsamplesToMatrix <- function(samples) {
  
  return (
    # t(
    #   sapply(samples,function(x) sapply(x,as.vector))
    #   )
      as.data.frame(t(sapply(samples,unlist)))
  )
}
# samplesS<-t(sapply(testAS,function(x) t(as.vector(x$S))))
# samplesS<-sapply(testAS,function(x) sapply(x,as.vector), simplify = FALSE)
# samplesS<-t(sapply(testAS,function(x) sapply(x,as.vector)))

legends <- c("A11","A12","A21","A31","S11","S12","S21","S22")
samplesAS<-nmfsamplesToMatrix(testAS)
# samplesS
```

```{r}
matplot(samplesAS[1:100,], type="b")
legend("topright",fill = 1:8, 280, 1, legend=legends,
       col=1:8, lty=1:3, cex=0.8)
```


The plot above shows that the Gibbs sampler quickly moves towards a particular solution for the matrices $A$ and $S$. Looking at the convergence plot in more detail, the sampler seems to move very slowly. This shows that the Gibbs sampler is very inefficient. 

```{r}
plot(samplesAS[-(1:50),1], type="l")
```
```{r}
plot(samplesAS[-(1:50),2], type="l",col=2)
```
```{r}
plot(samplesAS[-(1:50),3], type="l",col=3)
```

```{r}
plot(samplesAS[-(1:50),4], type="l",col=4)
```
```{r}
leg <- c("A11","A12","A21","A22")
par(mfrow=c(2,2))
for (i in 1:4){
  plot(samplesAS[-(1:50),i], type="l",col=i,main=leg[i])
}
```


**TODO:** Generate more convergence plots!
```{r}
leg <- c("S11","S12","S21","S22")
par(mfrow=c(2,2))
for (i in 5:8){
  plot(samplesAS[-(1:50),i], type="l",col=i,main=leg[i-4])
}
```
#README: From the analysis of the convergence plot it seems like a random walk data. Some convergence plot are 
#monotonously increasing eg: A21 and S12. While some are monotonously decreasing like: S22, A21. But other parameters are
#showing random pattern like some times the vaues are increasing and then decreasing. 

As an example we generate a histogram for the 8th parameter which happens to be the matrix component $a_{22}$.
```{r}
hist(samplesAS[-(1:50),8], main = colnames(samplesAS)[8])
```

Despite the large number of samples, the shape of the histogram is very irregular. This is likely due to the fact that the Gibbs sampler samples quite inefficiently.

**TODO:** Generate all eight histograms for all components of the matrices $A$ and $S$. 
```{r}
#Histogram for S
par(mfrow=c(2,2))
hist(samplesAS$S1[-(1:50)],main="Histogram of S11")
hist(samplesAS$S2[-(1:50)], main="Histogram of S12")
hist(samplesAS$S3[-(1:50)],main="Histogram of S21")
hist(samplesAS$S4[-(1:50)], main="Histogram of S22")
```
```{r}
#Histogram for A
par(mfrow=c(2,2))
hist(samplesAS$A1[-(1:50)], main="Histogram of A11")
hist(samplesAS$A2[-(1:50)], main="Histogram of A12")
hist(samplesAS$A3[-(1:50)],main="Histogram of A21")
hist(samplesAS$A4[-(1:50)], main="Histogram of A22")
```

We now compare with the results we obtain from `nmf`:

```{r}
NMFresults <- nmf(t(testData$X), 2)
res.A<-coef(NMFresults)
res.S<-basis(NMFresults)
t(res.A)
```

```{r}
t(res.S)
t(res.A)
```

```{r}
#testData$X - t(res.S%*%res.A)
testData$X - t(res.A)%*%t(res.S)

```
```{r}
result<- nmf.nextGibbsAS(testData$X, testData$A, testData$S)

result
```
```{r}
testData$X-result$A%*%result$S
```
The value of A and S sampled from gibbs sampler is totally different from that of the NMF but the product of A and S in both cases are very close to that of test data. Infact the difference of the product of A and S obtained from NMF and test data is 0 while the value is close to 0 for gibbs sampler suggesting that gibbs sampler is less precise or contains some noise. 

We now calcuate the means of the sampler for parametrising the matrices $A$ and $S$:

```{r}
samplesASBurnIn<-samplesAS[-(1:50),]
samplesASMeans<- colMeans(samplesASBurnIn)
samplesASMeans
```

We now fill the matrices $A$ and $S$ with these means 


```{r}
samplesAS.S <- matrix(samplesASMeans[1:4], nrow = 2, ncol = 2, byrow = FALSE)
samplesAS.S
```

```{r}
samplesAS.A <- matrix(samplesASMeans[5:8], nrow = 2, ncol = 2, byrow = FALSE)
samplesAS.A
```

```{r}
testData$X - testData$A%*%testData$S
```

The result is consistent with the noise levels $\boldsymbol{\sigma}=(\sigma_1,\sigma_2)=(0.1,0.1)$. 

### Even simpler test data

We now investigate at test data set that should have a unique solution - $A$ and $S$ are both diagonal (this is an old example, so only $S$ is sampled, assuming that $A$ is already known).

```{r}
simpleS<-diag(c(1,2))
simpleA<-diag(c(2,1))
simpleX <- simpleA%*%simpleS
simpleXerr<-simpleX
for(i in 1:nrow(simpleX)) {
    simpleXerr[i,] <- simpleX[i, ] + rnorm(ncol(simpleX), 
                                  mean=0,
                                  sd=0.1)
}
simpleXerr[simpleXerr<=0]<-0
simpleTest<-list(X=simpleXerr, S=simpleS, A=simpleA)
simpleTest
```

The solution is found by the `nmf` function from the `NMF` package.

```{r}
simpleNMFresults <- nmf(t(simpleTest$X), 2)
simpleA<-coef(simpleNMFresults)
simpleS<-basis(simpleNMFresults)
t(simpleA)
```

```{r}
t(simpleS)
```


```{r}
simpleTest$X - simpleTest$A%*%simpleTest$S
```

```{r}
#t(simpleA)%*%t(simpleS)
t(simpleS%*%simpleA)
```

```{r}
simpleTest$X
```


```{r}
## simpleTest$X - t(simpleA%*%simpleS)
simpleTest$X - t(simpleS%*%simpleA)
```

```{r}
N<-2000
samples <- vector(mode = "list", length = N)
  
samples[[1]] <- t(simpleS)

for(i in 2:N) {
  samples[[i]]<-nmf.sampleS(simpleTest$X,A = simpleTest$A, Sold = samples[[i-1]],sigma = rep(0.1,2))
}
```

```{r}
samplesMatrix <-t(sapply(samples, function(x) t(as.vector(x))))
# t(as.vector(samples[[1]]))
# matplot(samplesMatrix, type="b")
layout(matrix(1:4,nrow = 2,ncol = 2, byrow = TRUE))
hist(samplesMatrix[,1])
hist(samplesMatrix[,2])
hist(samplesMatrix[,3])
hist(samplesMatrix[,4])
```


The histograms show that $S$ is close to 

\[
S = \begin{pmatrix} 1 &0\\ 
0 &2
\end{pmatrix}
\]

as it should.

We generate matrix plots for identifying possible correlation between parameters:

```{r}
pairs(samplesMatrix, pch=".")
```

Also the column means are close to the expected values:
```{r}
colMeans(samplesMatrix)
```

And the standard deviations are on the order of magnitude of what is expected from the noise added:
```{r}
apply(samplesMatrix, 2, sd)
```

```{r}
testsimpleS<-nmfGibbsAS(simpleTest$X, simpleTest$A, simpleTest$S, N=1000)

testsimpleS[[100]]
```

## *Case 2:* Sampling the hyperparameter $\sigma_i$:

This code includes the sampling of the noise variances $\sigma_i^2$ in addition to the matrices $A$ and $S$. 

```{r}
nmf.sampleInverseSigmaSqr <- function(X, A, S, aPrior=1, bPrior=1e-3) {
  invSigma <- numeric(nrow(X))
  aPost <- ncol(X)/2 + aPrior
  residualsSqr <- ((X-A%*%S)^2)
  bPost <- rowSums(residualsSqr)/2 + bPrior
  for (i in 1:nrow(X)) {
    invSigma[i] <- rgamma(1, shape = aPost, rate = bPost[i])
  }
  return(invSigma)
}

nmf.sampleInverseSigmaSqr(testData$X,testData$A, testData$S,
                          aPrior = 1, bPrior = 1e-3)
```
```{r}
nmf.nextGibbsASSigma <- function(X, A, S, sigma,
                            alpha=rep(1, ncol(X)),
                            beta=rep(1e-3, ncol(X)),
                            gamma=rep(1, ncol(X)),
                            lambda=rep(1e-3, ncol(X)),
                            sigAlpha=1,sigBeta=1e-3) {
  nextS <- nmf.sampleS(X,A,S,sigma,alpha, beta)
  
  nextA <- nmf.sampleA(X, A, nextS, sigma, gamma, lambda)

  nextInvSigmaSqr <- nmf.sampleInverseSigmaSqr(X,nextA,nextS, 
                                               sigAlpha, sigBeta)
  
  return ( list(S=nextS, A=nextA, sigma=1/sqrt(nextInvSigmaSqr)))
}

nmf.nextGibbsASSigma(testData$X, testData$A, testData$S, rep(0.1,2))
```

```{r}
nmfGibbsASsigma <- function(X, A, S, sigma,
                            alpha=rep(1, ncol(X)),
                            beta=rep(1e-3, ncol(X)),
                            gamma=rep(1, ncol(X)),
                            lambda=rep(1e-3, ncol(X)),
                            sigAlpha=1, sigBeta=1e-3,
                            N=10) {
  currSample <- list(S=S, A=A, sigma=sigma)
  out <-list(currSample)
  
  for(i in 2:(N+1)) {
    currSample <- nmf.nextGibbsASSigma(
      X, currSample$A, currSample$S, currSample$sigma,
      alpha, beta, gamma, lambda, sigAlpha, sigBeta)
    out[[i]] <- currSample
  }
  return(out)
}

testASsigma <- nmfGibbsASsigma(testData$X, testData$A, testData$S, sigma = rep(0.1,2), sigBeta=1e-3, N=10000)
testASsigma[[length(testASsigma)]]
```


```{r}
samplesASsigma <- nmfsamplesToMatrix(testASsigma)
matplot(samplesASsigma[,9:10], type = "b")
```

```{r}
hist(samplesASsigma$sigma1[-(1:2000)])
```
```{r}
hist(samplesASsigma$sigma2[-(1:2000)])
```


```{r}
mean(samplesASsigma$sigma1[-(1:2000)])
```
```{r}

```

```{r}
testASsigma <- nmfGibbsASsigma(testData$X, 
                               A=matrix(rep(1000,4),nrow = 2), 
                               S=matrix(rep(1000,4), nrow = 2), 
                               sigma = rep(1,2), sigBeta=1e-3, N=100000)
lastAS<-testASsigma[[length(testASsigma)]]

lastAS
```


```{r}
samplesASsigma <- nmfsamplesToMatrix(testASsigma)
matplot(samplesASsigma[,1:8], type = "b",xlim = c(0,10000)#, ylim=c(0,80000)
        )
```


```{r}
burnin <- 10000
samplewithoutburnin <- samplesASsigma[-(1:burnin),]
matplot(samplewithoutburnin[,1:8], type = "b")
```

```{r}
leg <- c("S11","S12","S21","S22")
par(mfrow=c(2,2))
for (i in 1:4){
  plot(samplewithoutburnin[,i], type="l",col=i,main=leg[i])
}
```

```{r}
leg <- c("A11","A12","A21","A22")
par(mfrow=c(2,2))
for (i in 5:8){
  plot(samplewithoutburnin[,i], type="l",col=i,main=leg[i-4])
}
```

```{r}
leg <- c("Sigma1","Sigma2")
par(mfrow=c(1,2))
for (i in 9:10){
  plot(samplewithoutburnin[,i], type="l",col=i,main=leg[i-8])
}
```


```{r}
#Histogram for S
par(mfrow=c(2,2))
hist(samplewithoutburnin[,1], main = "Histogram of S11")
hist(samplewithoutburnin[,2], main = "Histogram of S12")
hist(samplewithoutburnin[,3], main = "Histogram of S21")
hist(samplewithoutburnin[,4], main = "Histogram of S22")
```
```{r}
#Histogram for A
par(mfrow=c(2,2))
hist(samplewithoutburnin[,5], main = "Histogram of A11")
hist(samplewithoutburnin[,6], main = "Histogram of A12")
hist(samplewithoutburnin[,7], main = "Histogram of A21")
hist(samplewithoutburnin[,8], main = "Histogram of A22")
```
```{r}
matplot(samplewithoutburnin[,9:10])
```

```{r}
par(mfrow=c(1,2))
hist(log(samplewithoutburnin[,9], base = 10), main = "Histogram of Sigma1")
hist(log(samplewithoutburnin[,10], base = 10), main = "Histogram of Sigma2")
```


```{r}
samplesASsigma <- nmfsamplesToMatrix(testASsigma)
matplot(samplesASsigma[,9:10], type = "b")
```

```{r}
layout(matrix(1:2,nrow = 2,ncol = 2, byrow = TRUE))
matplot(samplesASsigma[,1:8], type = "b", ylim=c(0,80000))
matplot(samplesASsigma[,9:10], type = "b")
```



```{r}
#Image Dataset Data Reading & Model
library(NMF)

library(imager)

library(tidyverse)

imgs <- load.dir("imgs_bss")

img_mat <- map_df(imgs, as.vector) %>% as.matrix() %>% t()

mdl_nmf <- nmf(t(img_mat), 4, "brunet")

S <- basis(mdl_nmf)
A <- coef(mdl_nmf)
```


```{r}
#Image Dataset
img_matA <- matrix(runif(n=nrow(img_mat)*4), nrow = nrow(img_mat))
img_matS <- matrix(runif(n=ncol(img_mat)*4), ncol = ncol(img_mat))
#AS <- nmf.nextGibbsAS(img_mat, img_matA, img_matS)
AS <- nmf.nextGibbsAS(img_mat, t(A), t(S))
AS 
```

```{r}
map_il(1:4, ~ {
  t(AS$S)[,.x] %>% as.vector %>% as.cimg(x=50,y=50)
}) %>% 
  imappend(axis = "x") %>% 
  plot(axes = F)
```


```{r}
# 10 iterations on Image Dataset
#testASsigma_img <- nmfGibbsASsigma(img_mat, t(A), t(S), sigma = rep(0.1,nrow(img_mat)), sigBeta=1e-3, N=10)
```

```{r}
#save(testASsigma_img, file = "testASsigma_ten")
```


```{r}
# Loading the results of 10 iterations on Image Dataset
testtenitterations_img <- load("testASsigma_ten")
```

```{r}
firsttestASsigma_img <- testASsigma_img[1][[1]]
thirdtestASsigma_img <- testASsigma_img[3][[1]]
fifthtestASsigma_img <- testASsigma_img[5][[1]]
seventestASsigma_img <- testASsigma_img[7][[1]]
ninetestASsigma_img <- testASsigma_img[9][[1]]
tenthtestASsigma_img <- testASsigma_img[10][[1]]
```

```{r}

firsttestASsigma_imgresult <- map_il(1:4, ~ {
  t(firsttestASsigma_img$S)[,.x] %>% as.vector %>% as.cimg(x=50,y=50)
}) %>% 
  imappend(axis = "x") %>% 
  plot(axes = F)

thirdtestASsigma_imgresult <- map_il(1:4, ~ {
  t(thirdtestASsigma_img$S)[,.x] %>% as.vector %>% as.cimg(x=50,y=50)
}) %>% 
  imappend(axis = "x") %>% 
  plot(axes = F)

fifthtestASsigma_imgresult <- map_il(1:4, ~ {
  t(fifthtestASsigma_img$S)[,.x] %>% as.vector %>% as.cimg(x=50,y=50)
}) %>% 
  imappend(axis = "x") %>% 
  plot(axes = F)

seventestASsigma_imgresult <- map_il(1:4, ~ {
  t(seventestASsigma_img$S)[,.x] %>% as.vector %>% as.cimg(x=50,y=50)
}) %>% 
  imappend(axis = "x") %>% 
  plot(axes = F)

ninetestASsigma_imgresult <- map_il(1:4, ~ {
  t(ninetestASsigma_img$S)[,.x] %>% as.vector %>% as.cimg(x=50,y=50)
}) %>% 
  imappend(axis = "x") %>% 
  plot(axes = F)

tenthtestASsigma_imgresult <- map_il(1:4, ~ {
  t(tenthtestASsigma_img$S)[,.x] %>% as.vector %>% as.cimg(x=50,y=50)
}) %>% 
  imappend(axis = "x") %>% 
  plot(axes = F)
```
```{r}
#Source 1
for (i in 1:11) {
  print(testASsigma_img[[i]]$S[1,1000])
}
```


```{r}
#Source 2
for (i in 1:11) {
  print(testASsigma_img[[i]]$S[2,1000])
}
```

```{r}
#Source 3
for (i in 1:11) {
  print(testASsigma_img[[i]]$S[3,1000])
}
```
```{r}
#Source 4
for (i in 1:11) {
  print(testASsigma_img[[i]]$S[4,1000])
}
```


```{r}
#Source 1
y1 = list()
x1 = list()
y2 = list()
x2 = list()
y3 = list()
x3 = list()
y4 = list()
x4 = list()
for (i in 1:11) {
  x1 = append(x1,i)
  y1 = append(y1,testASsigma_img[[i]]$S[1,1000])
  x2 = append(x2,i)
  y2 = append(y2,testASsigma_img[[i]]$S[2,1000])
  x3 = append(x3,i)
  y3 = append(y3,testASsigma_img[[i]]$S[3,1000])
  x4 = append(x4,i)
  y4 = append(y4,testASsigma_img[[i]]$S[4,1000])
}
par(mfrow=c(2,2))
plot(x1, y1, main = "Plot for 10 Iterations - Source 1",
     xlab = "Iterations", ylab = "Pixel Values",
     pch = 19, frame = FALSE)
lines(x1, y1, type = "l", lty = 1, col = "1")
plot(x2, y2, main = "Plot for 10 Iterations - Source 2",
     xlab = "Iterations", ylab = "Pixel Values",
     pch = 19, frame = FALSE)
lines(x2, y2, type = "l", lty = 1, col = "2")
plot(x3, y3, main = "Plot for 10 Iterations - Source 3",
     xlab = "Iterations", ylab = "Pixel Values",
     pch = 19, frame = FALSE)
lines(x3, y3, type = "l", lty = 1, col = "3")
plot(x4, y4, main = "Plot for 10 Iterations - Source 4",
     xlab = "Iterations", ylab = "Pixel Values",
     pch = 19, frame = FALSE)
lines(x4, y4, type = "l", lty = 1, col = "4")
```
